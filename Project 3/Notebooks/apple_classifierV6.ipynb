{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758bb418-5288-4ab9-bae2-312ddef18aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             448\n",
      "              ReLU-2         [-1, 16, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 16, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]           4,640\n",
      "              ReLU-5         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 32, 56, 56]               0\n",
      "            Linear-7                  [-1, 128]      12,845,184\n",
      "              ReLU-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 32]           4,128\n",
      "             ReLU-10                   [-1, 32]               0\n",
      "           Linear-11                    [-1, 4]             132\n",
      "          Dropout-12                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 12,854,532\n",
      "Trainable params: 12,854,532\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 20.67\n",
      "Params size (MB): 49.04\n",
      "Estimated Total Size (MB): 70.28\n",
      "----------------------------------------------------------------\n",
      "Epoch 1/2: Train Loss: 1.4800, Test Loss: 1.1650, Test Accuracy: 62.86%\n",
      "Epoch 2/2: Train Loss: 1.2647, Test Loss: 1.2122, Test Accuracy: 57.14%\n",
      "2023-06-07 15:16:54\n",
      "Total duration: 00:01:06\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import ImageReadMode\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define constants\n",
    "image_size = 224\n",
    "batch_size = 8\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "num_classes = 4\n",
    "dropout = 0.5\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to read directories and label images\n",
    "def label_images_in_directories(main_directory):\n",
    "    label_names = []\n",
    "    image_files = []\n",
    "    for directory in os.listdir(main_directory):\n",
    "        sub_directory = os.path.join(main_directory, directory)\n",
    "        if os.path.isdir(sub_directory):\n",
    "            for filename in os.listdir(sub_directory):\n",
    "                image_file = os.path.join(sub_directory, filename)\n",
    "                if os.path.isfile(image_file) and filename.endswith(\".jpg\"):\n",
    "                    label_names.append(directory)\n",
    "                    image_files.append(image_file)\n",
    "\n",
    "    image_tensors = [torchvision.io.read_image(image, mode=ImageReadMode.UNCHANGED).to(torch.float32)/255 for image in image_files]\n",
    "    nr_of_images = len(image_tensors)\n",
    "\n",
    "    return label_names, image_tensors\n",
    "\n",
    "# Load and label the images in the training directory\n",
    "train_dir = \"./../../Project 3/data/Data_cleaning_step1_2/Train\"\n",
    "label_names, image_tensors = label_images_in_directories(train_dir)\n",
    "\n",
    "# Load and label the images in the test directory\n",
    "test_dir = \"./../../Project 3/data/Data_cleaning_step1_2/Test\"\n",
    "label_names, image_tensors = label_images_in_directories(test_dir)\n",
    "\n",
    "# Define the transformations for train data before entering the neural network\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomCrop(size=image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Define the transformations for test data before entering the neural network\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "dataset_size = len(train_dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for managing the data batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_size_nn, dropout):\n",
    "        super().__init__()\n",
    "        self.CNN_Apple = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * image_size_nn ** 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.CNN_Apple(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "image_size_nn = int(image_size/4)\n",
    "model = CNNModel(num_classes, image_size_nn, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "summary(model.to(device), input_size=(3, image_size, image_size))\n",
    "\n",
    "# Define the loss function, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "# Training function\n",
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(train_loader), accuracy\n",
    "\n",
    "# Test function for validation and test sets\n",
    "def test(model, criterion, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(loader), accuracy\n",
    "\n",
    "# Initialize the best loss variable with infinity\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, criterion, optimizer, train_loader, device)\n",
    "    test_loss, test_accuracy = test(model, criterion, test_loader, device)\n",
    "    val_loss, val_accuracy = test(model, criterion, val_loader, device)\n",
    "\n",
    "    scheduler.step(test_loss) \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    # save model \n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'best_model_6.pth')\n",
    "        with open(\"model_apple_7_x.pth\", 'wb') as f:\n",
    "            torch.save(model.fc_layers, f)\n",
    "\n",
    "# Set a time and date\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(timestamp)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "duration_str = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
    "print(f\"Total duration: {duration_str}\")\n",
    "        \n",
    "data = {\n",
    "    \"Timestamp\": timestamp,\n",
    "    \"Duration\": duration_str,\n",
    "    \"Model type\": \"resnet18\",\n",
    "    \"Dataset use\": os.path.basename(train_dir),\n",
    "    \"Image Resize\": str(image_size)+\"*\"+str(image_size),\n",
    "    \"Epochs\": num_epochs,\n",
    "    \"Learning rate\": learning_rate,\n",
    "    \"Batch size\": batch_size,\n",
    "    \"Train Accuracy\": f\"{train_accuracy:.2f}\",\n",
    "    \"Validation accuracy\": f\"{val_accuracy:.2f}\",\n",
    "    \"Test Accuracy\": f\"{test_accuracy:.2f}\",\n",
    "}\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(\"model_data.csv\"):\n",
    "    existing_data = pd.read_csv(\"model_data.csv\")\n",
    "    new_data = pd.concat([existing_data, pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "else:\n",
    "    new_data = pd.DataFrame(data, index=[0])\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "new_data.to_csv(\"model_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc14f1-c4f3-444c-8b98-3d5572d5c7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
