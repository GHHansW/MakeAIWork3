{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c0dd26-59a4-4776-b6ec-c302d67b5a26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2512729862.py, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 50\u001b[0;36m\u001b[0m\n\u001b[0;31m    -----------------------------------------------------------------\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define constants\n",
    "resnet_type = \"resnet18\"\n",
    "image_size = 224\n",
    "batch_size = 8\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "num_classes = 4\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and label the images in the directory\n",
    "train_dir = \"./../Project 3/apple_disease_classification/Train_clean\"\n",
    "test_dir = \"./../Project 3/apple_disease_classification/Test_clean\"\n",
    "\n",
    "# Define the transformations for train data before entering the neural network\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),                                    # Resize images to 224x224 pixels\n",
    "    transforms.RandomCrop(size=image_size),                                         # Randomly crop and resize images to 224x224 pixels\n",
    "    transforms.RandomHorizontalFlip(),                                              # Randomly flip the images horizontally\n",
    "    transforms.RandomRotation(30),                                                  # Randomly rotate the images by 30 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, saturation, and hue\n",
    "    transforms.ToTensor(),                                                          # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])                 # Normalize the images\n",
    "])\n",
    "\n",
    "# Define the transformations for test data before entering the neural network\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),                     # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),                                           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the images\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
    "test_dataset = ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "# Create DataLoaders for managing the data batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# # Create DataLoaders for training, validation, and test datasets\n",
    "# train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
    "# train_ratio = 0.8\n",
    "# val_ratio = 0.1\n",
    "# test_ratio = 0.1\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# dataset_size = len(train_dataset)\n",
    "# train_size = int(train_ratio * dataset_size)\n",
    "# val_size = int(val_ratio * dataset_size)\n",
    "# test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "#     train_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoaders for managing the data batches\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Resnet model\n",
    "model = resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(train_loader), accuracy\n",
    "\n",
    "\n",
    "def test(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(test_loader), accuracy\n",
    "\n",
    "def validate(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(val_loader), accuracy\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, criterion, optimizer, train_loader, device)\n",
    "    test_loss, test_accuracy = test(model, criterion, test_loader, device)\n",
    "    val_loss, val_accuracy = validate(model, criterion, val_loader, device)\n",
    "\n",
    "    scheduler.step(test_loss) \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'Resnet_Weigth_Model.pth')\n",
    "\n",
    "        \n",
    "# Set a time and date\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(timestamp)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "duration_str = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
    "print(f\"Total duration: {duration_str}\")\n",
    "        \n",
    "data = {\n",
    "    \"Timestamp\": timestamp,\n",
    "    \"Duration\": duration_str,\n",
    "    \"Model type\": resnet_type,\n",
    "    \"Dataset use\": os.path.basename(train_dir),\n",
    "    \"Image Resize\": str(image_size)+\"*\"+str(image_size),\n",
    "    \"Epochs\": num_epochs,\n",
    "    \"Learning rate\": learning_rate,\n",
    "    \"Batch size\": batch_size,\n",
    "    \"Train Accuracy\": f\"{train_accuracy:.2f}\",\n",
    "    \"Validation accuracy\": f\"{val_accuracy:.2f}\",\n",
    "    \"Test Accuracy\": f\"{test_accuracy:.2f}\",\n",
    "}\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(\"model_data.csv\"):\n",
    "    existing_data = pd.read_csv(\"model_data.csv\")\n",
    "    new_data = pd.concat([existing_data, pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "else:\n",
    "    new_data = pd.DataFrame(data, index=[0])\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "new_data.to_csv(\"model_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330481a-5390-43b7-9f8b-d3396a2caa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
